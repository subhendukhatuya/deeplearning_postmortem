{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "- s1: Implement RNN **with** Default Pytorch `nn.RNN` module\n",
    "- s2: Implement RNN manually **without** `nn.RNN` module\n",
    "\n",
    "- s3: Pass same weight of default weight and bias of s1 to s2.\n",
    "- s4: pass same input_x to both s1 and s2\n",
    "\n",
    "- s5: Compare RNN output of **1 forward pass**, taking care of sequence length, for both `s1` and `s2`\n",
    "\n",
    "- [PYTORCH](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x104a7cf50>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking documentation from `pytorch/nn.RNN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = 4\n",
    "hidden_size = 3\n",
    "n_layers = 1\n",
    "seq_len = 5 #*****\n",
    "batch_sz = 1  #***\n",
    "\n",
    "torch.manual_seed(2)\n",
    "rnn = torch.nn.RNN(input_size, hidden_size, n_layers)\n",
    "input_x = torch.randn(seq_len, batch_sz, input_size)\n",
    "h0 = torch.randn(n_layers, batch_sz, hidden_size)\n",
    "ht, hn = rnn(input_x, h0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input_X\n",
    "\n",
    "The following input is passed to all the comparing RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0919, -0.1320, -0.2751, -0.2350]],\n",
       "\n",
       "        [[-1.2034, -1.0190,  0.3157, -1.6036]],\n",
       "\n",
       "        [[ 1.8493,  0.0447,  1.5853, -0.5912]],\n",
       "\n",
       "        [[ 0.1694,  0.7562, -1.2023, -0.5833]],\n",
       "\n",
       "        [[-0.4407, -1.9791,  0.7787, -0.7749]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1: RNN with `nn.RNN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_with_rnn_module(nn.Module):\n",
    "    def __init__(self,input_size,output_size,hidden_dim,n_layers):\n",
    "        super(RNN_with_rnn_module,self).__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.rnn=nn.RNN(input_size,hidden_dim,n_layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        hidden=torch.Tensor([[[0.5602, 0.9671, 0.2931]]]) #self.init_hidden(batch_size)\n",
    "        out,hidden=self.rnn(x,hidden)\n",
    "        return out,hidden\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        hidden=torch.zeros(self.n_layers,batch_size,self.hidden_dim)\n",
    "        hidden=torch.Tensor([[[0.5602, 0.9671, 0.2931]]])\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rnn.weight_ih_l0', Parameter containing:\n",
      "tensor([[ 0.1324, -0.1374,  0.1583, -0.0295],\n",
      "        [ 0.2466,  0.1375, -0.0664, -0.4668],\n",
      "        [ 0.1318, -0.5112,  0.0759,  0.0384]], requires_grad=True)), ('rnn.weight_hh_l0', Parameter containing:\n",
      "tensor([[-0.1270,  0.4721,  0.0385],\n",
      "        [ 0.2394,  0.2443, -0.3406],\n",
      "        [-0.2220,  0.5552, -0.5655]], requires_grad=True)), ('rnn.bias_ih_l0', Parameter containing:\n",
      "tensor([-0.0392, -0.0458,  0.4095], requires_grad=True)), ('rnn.bias_hh_l0', Parameter containing:\n",
      "tensor([-0.0549,  0.1520, -0.0277], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "rnn_with_module=RNN_with_rnn_module(input_size=4,output_size=4,hidden_dim=3,n_layers=1)\n",
    "print (list(rnn_with_module.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing with the above weight with below**\n",
    "\n",
    "```py\n",
    "[('linear_ip.weight', Parameter containing:\n",
    "tensor([[ 0.1324, -0.1374,  0.1583, -0.0295],\n",
    "        [ 0.2466,  0.1375, -0.0664, -0.4668],\n",
    "        [ 0.1318, -0.5112,  0.0759,  0.0384]], requires_grad=True)), ('linear_ip.bias', Parameter containing:\n",
    "tensor([-0.0392, -0.0458,  0.4095], requires_grad=True)), ('linear_hidden.weight', Parameter containing:\n",
    "tensor([[-0.1270,  0.4721,  0.0385],\n",
    "        [ 0.2394,  0.2443, -0.3406],\n",
    "        [-0.2220,  0.5552, -0.5655]], requires_grad=True)), ('linear_hidden.bias', Parameter containing:\n",
    "tensor([-0.0549,  0.1520, -0.0277], requires_grad=True))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1: After 1st forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2655,  0.4333,  0.5745]],\n",
      "\n",
      "        [[ 0.1750,  0.3546,  0.5104]],\n",
      "\n",
      "        [[ 0.5212,  0.6005,  0.5152]],\n",
      "\n",
      "        [[-0.1111,  0.6044, -0.1680]],\n",
      "\n",
      "        [[ 0.5069,  0.2106,  0.9488]]], grad_fn=<StackBackward>)\n",
      "tensor([[[0.5069, 0.2106, 0.9488]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Sending as a batch\n",
    "out, hidden_out=rnn_with_module(input_x)\n",
    "\n",
    "print(out)\n",
    "print(hidden_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2: RNN Manual implementation with n cell where `n=seq_len`\n",
    "\n",
    "Rememeber, while passing the hidden state to the next cell, make sure it's passed after the activation applied on it. Check line 23, 27, 28 to see how the h_n passed from one cell after another.\n",
    "\n",
    "![image](https://datascience-enthusiast.com/figures/rnn.png)\n",
    "\n",
    "**Note:** For the below architecture, consider $a_t$ (from above image) is equal to $h_n$\n",
    "\n",
    "- [source_blog](https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNIMPLEMENT_multiple_cell(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,seq_len,output_size):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.linear_ip=nn.Linear(input_dim,hidden_dim)\n",
    "        self.linear_ip.weight.data=torch.Tensor([[ 0.1324, -0.1374,  0.1583, -0.0295],\n",
    "                                                    [ 0.2466,  0.1375, -0.0664, -0.4668],\n",
    "                                                    [ 0.1318, -0.5112,  0.0759,  0.0384]])\n",
    "        self.linear_ip.bias.data=torch.Tensor([-0.0392, -0.0458,  0.4095])\n",
    "               \n",
    "        self.linear_hidden=nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_hidden.weight.data=torch.Tensor([[-0.1270,  0.4721,  0.0385],\n",
    "                                                        [ 0.2394,  0.2443, -0.3406],\n",
    "                                                        [-0.2220,  0.5552, -0.5655]])\n",
    "        self.linear_hidden.bias.data=torch.Tensor([-0.0549,  0.1520, -0.0277])\n",
    "    \n",
    "        self.activation=torch.tanh\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x: [seq_len, input_dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        h_n=torch.Tensor([[[0.5602, 0.9671, 0.2931]]])\n",
    "        output = []\n",
    "        for i in range(self.seq_len):\n",
    "            linear_output=self.linear_ip(x[i])\n",
    "            hidden_output_interim=self.linear_hidden(h_n)\n",
    "            h_n=self.activation(linear_output+hidden_output_interim)\n",
    "            output.append(h_n)\n",
    "        \n",
    "        return output, h_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if the weights are same with S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear_ip.weight', Parameter containing:\n",
      "tensor([[ 0.1324, -0.1374,  0.1583, -0.0295],\n",
      "        [ 0.2466,  0.1375, -0.0664, -0.4668],\n",
      "        [ 0.1318, -0.5112,  0.0759,  0.0384]], requires_grad=True)), ('linear_ip.bias', Parameter containing:\n",
      "tensor([-0.0392, -0.0458,  0.4095], requires_grad=True)), ('linear_hidden.weight', Parameter containing:\n",
      "tensor([[-0.1270,  0.4721,  0.0385],\n",
      "        [ 0.2394,  0.2443, -0.3406],\n",
      "        [-0.2220,  0.5552, -0.5655]], requires_grad=True)), ('linear_hidden.bias', Parameter containing:\n",
      "tensor([-0.0549,  0.1520, -0.0277], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "model_rnn_mc=RNNIMPLEMENT_multiple_cell(4,3,5,1)\n",
    "print (list(model_rnn_2.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2: After 1st forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[[0.2655, 0.4332, 0.5744]]], grad_fn=<TanhBackward>),\n",
       "  tensor([[[0.1750, 0.3546, 0.5104]]], grad_fn=<TanhBackward>),\n",
       "  tensor([[[0.5211, 0.6004, 0.5151]]], grad_fn=<TanhBackward>),\n",
       "  tensor([[[-0.1111,  0.6044, -0.1681]]], grad_fn=<TanhBackward>),\n",
       "  tensor([[[0.5069, 0.2105, 0.9489]]], grad_fn=<TanhBackward>)],\n",
       " tensor([[[0.5069, 0.2105, 0.9489]]], grad_fn=<TanhBackward>))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn_mc(input_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1. After 1st forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2655,  0.4333,  0.5745]],\n",
       " \n",
       "         [[ 0.1750,  0.3546,  0.5104]],\n",
       " \n",
       "         [[ 0.5212,  0.6005,  0.5152]],\n",
       " \n",
       "         [[-0.1111,  0.6044, -0.1680]],\n",
       " \n",
       "         [[ 0.5069,  0.2106,  0.9488]]], grad_fn=<StackBackward>),\n",
       " tensor([[[0.5069, 0.2106, 0.9488]]], grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_with_module(input_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark:\n",
    "\n",
    "As you can see both `model_rnn_mc(input_x)` and `rnn_with_module(input_x)` giving the same output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN without `nn.RNN` with single cell only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNIMPLEMENT_1(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_size):\n",
    "        super().__init__()\n",
    "        self.linear_ip=nn.Linear(input_dim,hidden_dim)\n",
    "        self.linear_hidden=nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.activation=torch.tanh\n",
    "    def forward(self,x):\n",
    "        h0=torch.Tensor([[[0.5602, 0.9671, 0.2931]]])\n",
    "        linear_output=self.linear_ip(x)\n",
    "        hidden_output=self.linear_hidden(h0)\n",
    "        \n",
    "        h_n=self.activation(linear_output+hidden_output)\n",
    "        return h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear_ip.weight', Parameter containing:\n",
      "tensor([[ 0.1147, -0.1190,  0.1371, -0.0255],\n",
      "        [ 0.2136,  0.1190, -0.0575, -0.4042],\n",
      "        [ 0.1142, -0.4427,  0.0657,  0.0332]], requires_grad=True)), ('linear_ip.bias', Parameter containing:\n",
      "tensor([-0.1099,  0.4088,  0.0334], requires_grad=True)), ('linear_hidden.weight', Parameter containing:\n",
      "tensor([[ 0.2394,  0.2443, -0.3406],\n",
      "        [-0.2220,  0.5552, -0.5655],\n",
      "        [-0.0392, -0.0458,  0.4095]], requires_grad=True)), ('linear_hidden.bias', Parameter containing:\n",
      "tensor([-0.0549,  0.1520, -0.0277], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "model_rnn_1=RNNIMPLEMENT_1(4,3,1)\n",
    "print (list(model_rnn_1.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Manual Implementation, without `nn.RNN`, with default weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNIMPLEMENT_single_cell(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_size):\n",
    "        super().__init__()\n",
    "        self.linear_ip=nn.Linear(input_dim,hidden_dim)\n",
    "        self.linear_ip.weight.data=torch.Tensor([[ 0.1324, -0.1374,  0.1583, -0.0295],\n",
    "                                                    [ 0.2466,  0.1375, -0.0664, -0.4668],\n",
    "                                                    [ 0.1318, -0.5112,  0.0759,  0.0384]])\n",
    "        self.linear_ip.bias.data=torch.Tensor([-0.0392, -0.0458,  0.4095])\n",
    "               \n",
    "        self.linear_hidden=nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_hidden.weight.data=torch.Tensor([[-0.1270,  0.4721,  0.0385],\n",
    "                                                        [ 0.2394,  0.2443, -0.3406],\n",
    "                                                        [-0.2220,  0.5552, -0.5655]])\n",
    "        self.linear_hidden.bias.data=torch.Tensor([-0.0549,  0.1520, -0.0277])\n",
    "    \n",
    "        self.activation=torch.tanh\n",
    "    def forward(self,x):\n",
    "        h0=torch.Tensor([[[0.5602, 0.9671, 0.2931]]])\n",
    "        linear_output=self.linear_ip(x)\n",
    "        \n",
    "        h_n=self.activation(linear_output+hidden_output)\n",
    "        return h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear_ip.weight', Parameter containing:\n",
      "tensor([[ 0.1324, -0.1374,  0.1583, -0.0295],\n",
      "        [ 0.2466,  0.1375, -0.0664, -0.4668],\n",
      "        [ 0.1318, -0.5112,  0.0759,  0.0384]], requires_grad=True)), ('linear_ip.bias', Parameter containing:\n",
      "tensor([-0.0392, -0.0458,  0.4095], requires_grad=True)), ('linear_hidden.weight', Parameter containing:\n",
      "tensor([[-0.1270,  0.4721,  0.0385],\n",
      "        [ 0.2394,  0.2443, -0.3406],\n",
      "        [-0.2220,  0.5552, -0.5655]], requires_grad=True)), ('linear_hidden.bias', Parameter containing:\n",
      "tensor([-0.0549,  0.1520, -0.0277], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "model_rnn_2=RNNIMPLEMENT_single_cell(4,3,1)\n",
    "print (list(model_rnn_2.named_parameters()))\n",
    "\n",
    "# input_x=torch.Tensor([[[ 0.0219, -0.3409, -1.1657,  0.8022]]])\n",
    "# model_rnn_2(input_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Checking with linear followed by tan (h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rnn.weight_ih_l0', Parameter containing:\n",
      "tensor([[ 0.1324, -0.1374,  0.1583, -0.0295],\n",
      "        [ 0.2466,  0.1375, -0.0664, -0.4668],\n",
      "        [ 0.1318, -0.5112,  0.0759,  0.0384]], requires_grad=True)), ('rnn.weight_hh_l0', Parameter containing:\n",
      "tensor([[-0.1270,  0.4721,  0.0385],\n",
      "        [ 0.2394,  0.2443, -0.3406],\n",
      "        [-0.2220,  0.5552, -0.5655]], requires_grad=True)), ('rnn.bias_ih_l0', Parameter containing:\n",
      "tensor([-0.0392, -0.0458,  0.4095], requires_grad=True)), ('rnn.bias_hh_l0', Parameter containing:\n",
      "tensor([-0.0549,  0.1520, -0.0277], requires_grad=True)), ('fc.weight', Parameter containing:\n",
      "tensor([[-0.3233, -0.3272, -0.2805]], requires_grad=True)), ('fc.bias', Parameter containing:\n",
      "tensor([-0.5245], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "class RNNMODEL(nn.Module):\n",
    "    def __init__(self,input_size,output_size,hidden_dim,n_layers):\n",
    "        super(RNNMODEL,self).__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.rnn=nn.RNN(input_size,hidden_dim,n_layers,batch_first=True,bias=True)\n",
    "        self.fc=nn.Linear(hidden_dim,output_size)\n",
    "    def forward(self,x):\n",
    "        h0=torch.Tensor([[[0.5602, 0.9671, 0.2931]]])\n",
    "        out,hidden=self.rnn(x,h0)\n",
    "        return out,hidden\n",
    "\n",
    "torch.manual_seed(2)\n",
    "RNNM=RNNMODEL(4,1,3,1)\n",
    "\n",
    "print(list(RNNM.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

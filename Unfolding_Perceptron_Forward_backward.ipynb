{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Source_blog](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Perceptron\n",
    "\n",
    "![image](https://matthewmazur.files.wordpress.com/2018/03/neural_network-9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(m):\n",
    "    if type(m) in [nn.Conv2d,nn.Linear]:\n",
    "        m.weight.data=torch.Tensor([[0.15,0.20],[0.25,0.30]])\n",
    "        m.bias.data=torch.Tensor([0.35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Set Up\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self,input_size,H1,output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear=nn.Linear(input_size,H1)\n",
    "        # manual weight initialization for input to hidden node [refer above image]\n",
    "        self.linear.weight.data=torch.Tensor([[0.15,0.20],[0.25,0.30]])\n",
    "        self.linear.bias.data=torch.Tensor([0.35])\n",
    "        \n",
    "        # manual weight initialization for input to hidden node [refer above image]\n",
    "        self.linear2=nn.Linear(H1,output_size)\n",
    "        self.linear2.weight.data=torch.Tensor([[0.40,0.45],[0.50,0.55]])\n",
    "        self.linear2.bias.data=torch.Tensor([0.60])\n",
    "    \n",
    "    def forward(self,x, print_values=False):\n",
    "\n",
    "        ## Actual Return after Sigmoid\n",
    "        net_h = self.linear(x)\n",
    "        out_h=torch.sigmoid(net_h)\n",
    "        net_O = self.linear2(out_h)\n",
    "        out_O = torch.sigmoid(net_O)\n",
    "        \n",
    "        if print_values:\n",
    "            print(\"h1: {}, h2: {}\".format(net_h[0], net_h[1]))\n",
    "            print(\"out_h1: {}, out_h2: {}\".format(out_h[0], out_h[1]))\n",
    "            print(\"net_O1: {}, net_O2: {}\".format(net_O[0], net_O[1]))\n",
    "            print(\"out_O1: {}, out_O2: {}\".format(out_O[0], out_O[1]))\n",
    "\n",
    "        \n",
    "        return (out_O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfold each epoch and check intermediate values\n",
    "\n",
    "\n",
    "### Initial Weight of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.1500, 0.2000],\n",
      "        [0.2500, 0.3000]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3500], requires_grad=True), Parameter containing:\n",
      "tensor([[0.4000, 0.4500],\n",
      "        [0.5000, 0.5500]], requires_grad=True), Parameter containing:\n",
      "tensor([0.6000], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model=Perceptron(2,2,1)\n",
    "print (list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After 1 forward pass check hidden, output and loss\n",
    "\n",
    "![image](blog_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.forward(torch.Tensor([0.05,0.10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = \\left( x_n - y_n \\right)^2,$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##??nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total MSEerror: 0.2983711063861847\n"
     ]
    }
   ],
   "source": [
    "model=Perceptron(2,2,2)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.5)\n",
    "output=model.forward(torch.Tensor([0.05,0.10]))\n",
    "target=torch.Tensor([0.01,0.99])\n",
    "loss=criterion(output,target)\n",
    "print(\"total MSEerror: {}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.1498, 0.1996],\n",
      "        [0.2498, 0.2995]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3406], requires_grad=True), Parameter containing:\n",
      "tensor([[0.3589, 0.4087],\n",
      "        [0.5113, 0.5614]], requires_grad=True), Parameter containing:\n",
      "tensor([0.5498], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# set prev grad to zero\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output = model.forward(torch.Tensor([0.05,0.10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.1496, 0.1992],\n",
      "        [0.2495, 0.2991]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3327], requires_grad=True), Parameter containing:\n",
      "tensor([[0.3171, 0.3666],\n",
      "        [0.5232, 0.5733]], requires_grad=True), Parameter containing:\n",
      "tensor([0.4991], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "loss=criterion(y_output,target)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinitializing the model and looping over Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.2983711063861847, output: tensor([0.7514, 0.7729], grad_fn=<SigmoidBackward>)\n",
      "============================================================\n",
      "Epoch: 10, loss: 0.17523802816867828, output: tensor([0.5335, 0.7135], grad_fn=<SigmoidBackward>)\n",
      "============================================================\n",
      "Epoch: 20, loss: 0.10270833224058151, output: tensor([0.3705, 0.7153], grad_fn=<SigmoidBackward>)\n",
      "============================================================\n",
      "Epoch: 30, loss: 0.06461693346500397, output: tensor([0.2801, 0.7528], grad_fn=<SigmoidBackward>)\n",
      "============================================================\n",
      "Epoch: 40, loss: 0.04308116436004639, output: tensor([0.2253, 0.7905], grad_fn=<SigmoidBackward>)\n",
      "============================================================\n",
      "Epoch: 50, loss: 0.03048398159444332, output: tensor([0.1893, 0.8202], grad_fn=<SigmoidBackward>)\n",
      "============================================================\n",
      "Epoch: 60, loss: 0.022741930559277534, output: tensor([0.1640, 0.8425], grad_fn=<SigmoidBackward>)\n",
      "============================================================\n",
      "Epoch: 70, loss: 0.017712488770484924, output: tensor([0.1455, 0.8594], grad_fn=<SigmoidBackward>)\n",
      "============================================================\n",
      "Epoch: 80, loss: 0.014273736625909805, output: tensor([0.1314, 0.8725], grad_fn=<SigmoidBackward>)\n",
      "============================================================\n",
      "Epoch: 90, loss: 0.01181731652468443, output: tensor([0.1203, 0.8829], grad_fn=<SigmoidBackward>)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "model=Perceptron(2,2,2)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.5)\n",
    "\n",
    "input_x = torch.Tensor([0.05,0.10])\n",
    "for i in range(100):\n",
    "    y_output=model.forward(input_x)\n",
    "    \n",
    "    loss=criterion(y_output,target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        print(f'Epoch: {i}, loss: {loss.item()}, output: {y_output}')\n",
    "        print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** at Line 3 above, if we use `Adam` optimizer, it will learn faster in lesser epoch. For example, with Adam, only using 10 epochs, the model will learn the optimum weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.1731, 0.2462],\n",
      "        [0.2726, 0.3452]], requires_grad=True), Parameter containing:\n",
      "tensor([1.2642], requires_grad=True), Parameter containing:\n",
      "tensor([[-1.1495, -1.1076],\n",
      "        [ 1.5021,  1.5570]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3011], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1111, 0.8918], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print (model.forward(torch.Tensor([0.06,0.12])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking gradients of the particular layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0080,  0.0080],\n",
      "        [-0.0076, -0.0076]])\n"
     ]
    }
   ],
   "source": [
    "print (model.linear2.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
